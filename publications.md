---
layout: page
permalink: /publications/index.html
title: Research
---

[[**Google Scholar**]](https://scholar.google.com/citations?hl=en&user=zea9MKUAAAAJ) [[**DBLP**]](https://dblp.org/pid/259/2327.html)  [[**Patent**]](https://www.patentguru.com/cn/inventor/%E5%BE%90%E6%9B%A6%E7%83%88) <br/>

## Research Statement
I have a strong interest in both the theories and practical applications of **trustworthy machine learning**, with a particular focus on **adversarial robustness**. 
Overall, my research works lie in the following three categories: <br/>
**(1) Towards developing and utilizing adversarially robust foundation models**: [[ICLR'24]](#autoRFT) [[NeurIPS'23a, *Spotlight*]](#NIPS23a), [[NeurIPS'23b]](#NIPS23b). <br/>
**(2) Towards evaluating and enhancing adversarial robustness of AI-powered applications (e.g., statistical tools, LLMs, diffusion models)**: [[ICLR'24]](#promptattack)[[AAAI'24 Workshop]](#AAAI24_ReLM), [[ICML'22]](#ICML22). <br/>
**(3) Towards enhancing supervised adversarial training**: [[TMLR'22]](#TMLR22), [[TDSC'22]](#TDSC22), [[ICML'20]](#ICML20).

<!-- I'm always welcoming the possibility of collaborations. Please feel free to contact me via [email](xuxilie@comp.nus.edu.sg) if you have any appropriate opportunities you'd like to explore. -->

<!-- ## Preprint -->
<!-- - <span id="autoRFT">AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework.</span> 
<br/> **Xilie Xu**, Jingfeng Zhang, Mohan Kankanhalli. 
<br/> *Preprint'23.*
<br/> [[PDF]](https://arxiv.org/abs/2310.01818) [[Code]](https://github.com/GodXuxilie/RobustSSL_Benchmark) -->

<!-- - <span id="promptattack">An LLM can Fool Itself: A Prompt-Based Adversarial Attack.</span> 
<br/> **Xilie Xu**, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. 
<br> *Preprint'23b.*
<br/> [[PDF]](https://arxiv.org/abs/2310.13345) [[Code]](https://github.com/GodXuxilie/PromptAttack)  -->


## Publication
(\* refers to equal contributions)
- <span id="autoRFT">AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework.</span> 
<br/> **Xilie Xu**, Jingfeng Zhang, Mohan Kankanhalli. 
<br/> [*The Twelfth International Conference on Learning Representations*](https://iclr.cc/Conferences/2024) *(ICLR 2024)*, Vienna, Austria, 2024.
<br/> [[PDF]](https://arxiv.org/abs/2310.01818) [[Code]](https://github.com/GodXuxilie/RobustSSL_Benchmark)

- <span id="promptattack">An LLM can Fool Itself: A Prompt-Based Adversarial Attack.</span> 
<br/> **Xilie Xu**, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. 
<br/> [*The Twelfth International Conference on Learning Representations*](https://iclr.cc/Conferences/2024) *(ICLR 2024)*, Vienna, Austria, 2024.
<br/> [[PDF]](https://arxiv.org/abs/2310.13345) [[Code]](https://github.com/GodXuxilie/PromptAttack) 

- <span id="AAAI24_ReLM">AdvGLUE-GPT: Towards Effective and Efficient Robustness Evaluation of Large Language Models.</span> 
<br/> **Xilie Xu**, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. 
<br> *AAAI Workshop on Responsible Language Models, Vancouver, Canada, 2024.*
<br/> [[PDF]](https://arxiv.org/abs/2310.13345) [[Code]](https://github.com/GodXuxilie/PromptAttack) [[Project Page]](https://godxuxilie.github.io/project_page/prompt_attack)

- <span id="NIPS23a">Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*The 37th Annual Conference on Neural Information Processing Systems*](https://neurips.cc/Conferences/2023) *(NeurIPS 2023)*, New Orleans, US, 2023. 
<br/> [[PDF]](https://arxiv.org/pdf/2302.03857.pdf) [[Code]](https://github.com/GodXuxilie/Efficient_ACL_via_RCS) [[Poster]](https://nips.cc/media/PosterPDFs/NeurIPS%202023/70886.png?t=1701436495.3604662) [[知乎 (Chinese Quora)]](https://zhuanlan.zhihu.com/p/669541942) [**Spotlight**]
<!-- [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:E_wpy3GVQbYJ:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkq7I:AFWwaeYAAAAAZRFis7KReHcROHAYnWoiV6gvx6Y&scisig=AFWwaeYAAAAAZRFisyCipyX6BVfYxxDG9kfCWu0&scisf=4&ct=citation&cd=-1&hl=en) -->

- <span id="NIPS23b">Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*The 37th Annual Conference on Neural Information Processing Systems*](https://neurips.cc/Conferences/2023) *(NeurIPS 2023)*, New Orleans, US, 2023. 
<br/> [[PDF]](https://arxiv.org/pdf/2305.00374.pdf) [[Code]](https://github.com/GodXuxilie/Enhancing_ACL_via_AIR) [[Poster]](https://nips.cc/media/PosterPDFs/NeurIPS%202023/69867.png?t=1701436551.2570322) [[Project Page]](https://robustssl.github.io/)
<!-- [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:OluEdScbo14J:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkhpk:AFWwaeYAAAAAZRFinplfA6oeNQMV6GB6ciligwg&scisig=AFWwaeYAAAAAZRFinrFEeKoc-BMy_xfCVD0W_W4&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1) -->

- <span id="ICML22">Adversarial Attack and Defense for Non-Parametric Two-Sample Tests.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*The 39th International Conference on Machine Learning*](https://icml.cc/Conferences/2022) *(ICML 2022)*, Baltimore, US, 2022. <br/> [[PDF]](https://proceedings.mlr.press/v162/xu22m/xu22m.pdf) [[Code]](https://github.com/GodXuxilie/Robust-TST) 
<!-- [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:2g1wRPv3Id4J:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkYEA:AFWwaeYAAAAAZRFieEDgzxiUY3BXxqs_xZL1MgE&scisig=AFWwaeYAAAAAZRFieI9pa-Q3utJ9CQwwgSiJ31I&scisf=4&ct=citation&cd=-1&hl=en) -->

- <span id="TMLR22">NoiLin: Improving Adversarial Training and Correcting Stereotype of Noisy Labels. </span> 
<br> Jingfeng Zhang\*, **Xilie Xu\***, Bo Han, Tongliang Liu, Lizhen Cui, Gang Niu, Masashi Sugiyama. 
<br/> [*Transactions on Machine Learning Research*](https://jmlr.org/tmlr/) *(TMLR 2022)*. 
<br/> [[PDF]](https://openreview.net/pdf?id=zlQXV7xtZs) [[Code]](https://github.com/zjfheart/NoiLIn) 
<!-- [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:XgdVUPGCD5oJ:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkfhk:AFWwaeYAAAAAZRFiZhm0iLKjrkkSgMZj9OXzurs&scisig=AFWwaeYAAAAAZRFiZtcATe8SWP8hwQHf88b1n-E&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1) -->


- <span id="TDSC22">Decision Boundary-aware Data Augmentation for Adversarial Training.</span> 
<br> Chen Chen\*, Jingfeng Zhang\*, **Xilie Xu**, Lingjuan Lyu, Chaochao Chen, Tianlei Hu, Gang Chen. 
<br/> [*IEEE Transactions on Dependable and Secure Computing*](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8858) *(TDSC 2022)*. 
<br/> [[PDF]](https://ieeexplore.ieee.org/abstract/document/9754227) 
<!-- [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:ZJT4e3wdL3YJ:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkUgk:AFWwaeYAAAAAZRFiSgk8vu679Kz5vSH7IboyCFA&scisig=AFWwaeYAAAAAZRFiSk7NLwlZHp3poWY2WEnqIqs&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1) -->


- <span id="ICML20">Attacks Which Do Not Kill Training Make Adversarial Learning Stronger.</span> 
<br/> Jingfeng Zhang\*, **Xilie Xu\***, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*The 37th International Conference on Machine Learning*](https://icml.cc/Conferences/2020) *(ICML 2022)*, Online, 2020.  <br/> [[PDF]](https://proceedings.mlr.press/v119/zhang20z/zhang20z.pdf) [[Code]](https://github.com/zjfheart/Friendly-Adversarial-Training) 
<!-- [[BibTeX]]() -->

