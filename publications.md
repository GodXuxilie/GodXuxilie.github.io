---
layout: page
permalink: /publications/index.html
title: Research
---

[[**Google Scholar**]](https://scholar.google.com/citations?hl=en&user=zea9MKUAAAAJ) [[**DBLP**]](https://dblp.org/pid/259/2327.html)  [[**Patent**]](https://www.patentguru.com/cn/inventor/%E5%BE%90%E6%9B%A6%E7%83%88) <br/>

## Research Statement
I have a strong interest in both the theories and practical applications of **trustworthy machine learning**, with a particular focus on **adversarial robustness**. 
Overall, my research works lie in the following three categories: <br/>
**(1) Towards developing and fine-tuning trustworthy foundation models**: [[AAAI'25]](#PrivateLoRA), [[NeurIPS'24]](#robust_alignment), [[ICLR'24 Blogpost]](#ICLR24_blogpost), [[ICLR'24a]](#autoRFT), [[NeurIPS'23a, *Spotlight*]](#NIPS23a), [[NeurIPS'23b]](#NIPS23b). <br/>
**(2) Towards evaluating and enhancing adversarial robustness of AI-powered applications (e.g., statistical tools, LLMs, diffusion models)**: [[ICML'24 Challenge Chanpionship]](#TiFA), [[ICLR'24b]](#promptattack), [[AAAI'24 Workshop]](#AAAI24_ReLM), [[ICML'22]](#ICML22). <br/>
**(3) Towards enhancing supervised adversarial training**: [[TMLR'22]](#TMLR22), [[TDSC'22]](#TDSC22), [[ICML'20]](#ICML20).

<!-- I'm always welcoming the possibility of collaborations. Please feel free to contact me via [email](xuxilie@comp.nus.edu.sg) if you have any appropriate opportunities you'd like to explore. -->

## Preprint & Workshop Paper & Blogpost & Challenge
(\* refers to equal contributions) 
<ol reversed>
<li><span id="TiFA">Technical Report for ICML 2024 TiFA Workshop MLLM Attack Challenge: Suffix Injection and Projected Gradient Descent Can Easily Fool An MLLM.</span> <br/> 
Yangyang Guo, Ziwei Xu, <b>Xilie Xu</b>, Yongkang Wong, Liqiang Nie, Mohan Kankanhalli.<br> 
ICML 2024 TiFA Workshop MLLM Attack Challenge, Vienna, Austria, 2024.
<br/> [<a href="https://godxuxilie.github.io/file/ICML-TiFA-Certificate.jpg">Championship Certification</a>]
</li>
<li><span id="ICLR24_blogpost">Towards Robust Foundation Models: Adversarial Contrastive Learning.</span><br/> 
Jingfeng Zhang, <b>Xilie Xu</b>. <br> 
<a href="https://openreview.net/group?id=ICLR.cc/2024/BlogPosts#tab-accept"><i>The Third Blogpost Track at ICLR 2024</i></a> <i>(BT@ICLR 2024)</i>, Vienna, Austria, 2024.
<br/> [<a href="https://iclr-blogposts.github.io/2024/blog/robust-foundation-model/">Blogpost</a>]
</li>
<li><span id="AAAI24_ReLM">AdvGLUE-GPT: Towards Effective and Efficient Robustness Evaluation of Large Language Models.</span> <br/> 
<b>Xilie Xu</b>, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. <br> 
<a href="https://sites.google.com/vectorinstitute.ai/relm2024/home"><i>AAAI Workshop on Responsible Language Models</i></a>, Vancouver, Canada, 2024.
<br/> [<a href="https://arxiv.org/abs/2310.13345">PDF</a>] [<a href="https://github.com/GodXuxilie/PromptAttack">Code</a>]</li>
</ol>

## Publication
<ol reversed>
<li><span id="PrivateLoRA">Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models.</span> <br/> 
Zihao Luo<sup>*</sup>, <b>Xilie Xu</b><sup>*</sup>, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang. <br> 
<a href="https://aaai.org/conference/aaai/aaai-25/"><i>The 39th AAAI Conference on Artificial Intelligence</i></a> <i>(AAAI 2025)</i>, Philadelphia, US, 2025.
<br/> [<a href="https://arxiv.org/abs/2402.11989">PDF</a>] [<a href="https://github.com/WilliamLUO0/StablePrivateLoRA">Code</a>]
</li>
<li><span id="robust_alignment">Perplexity-aware Correction for Robust Alignment with Noisy Preferences.</span><br/> 
Keyi Kong<sup>*</sup>, <b>Xilie Xu</b><sup>*</sup>, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. <br> 
<a href="https://neurips.cc/Conferences/2024"><i>The 38th Annual Conference on Neural Information Processing Systems</i></a> <i>(NeurIPS 2024)</i>, Vancouver, Canada, 2024.
<br/> [<a href="https://openreview.net/pdf?id=OUXnnPJzXJ">PDF</a>] [<a href="https://github.com/luxinyayaya/PerpCorrect">Code</a>]
</li>
<li><span id="promptattack">An LLM can Fool Itself: A Prompt-Based Adversarial Attack.</span><br/> 
<b>Xilie Xu</b><sup>*</sup>, Keyi Kong<sup>*</sup>, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. <br> 
<a href="https://iclr.cc/Conferences/2024"><i>The 12th International Conference on Learning Representations</i></a> <i>(ICLR 2024)</i>, Vienna, Austria, 2024.
<br/> [<a href="https://arxiv.org/abs/2310.13345">PDF</a>] [<a href="https://github.com/GodXuxilie/PromptAttack">Code</a>] [<a href="https://iclr.cc/media/PosterPDFs/ICLR%202024/18503.png?t=1712917706.7582233">Poster</a>] [<a href="https://godxuxilie.github.io/project_page/prompt_attack">Project Page</a>]
</li>
<li><span id="autoRFT">AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework.</span><br/>
<b>Xilie Xu</b>, Jingfeng Zhang, Mohan Kankanhalli. <br> 
<a href="https://iclr.cc/Conferences/2024"><i>The 12th International Conference on Learning Representations</i></a> <i>(ICLR 2024)</i>, Vienna, Austria, 2024.
<br/> [<a href="https://arxiv.org/abs/2310.01818">PDF</a>] [<a href="https://github.com/GodXuxilie/RobustSSL_Benchmark">Code</a>] [<a href="https://iclr.cc/media/PosterPDFs/ICLR%202024/19622.png?t=1713170951.8301408">Poster</a>]
</li>
<li><span id="NIPS23a">Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.</span> <br/> 
<b>Xilie Xu</b><sup>*</sup>, Jingfeng Zhang<sup>*</sup>, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. <br> 
<a href="https://neurips.cc/Conferences/2023"><i>The 37th Annual Conference on Neural Information Processing Systems</i></a> <i>(NeurIPS 2023)</i>,  New Orleans, US, 2023. 
<br/> [<a href="https://arxiv.org/pdf/2302.03857.pdf">PDF</a>] [<a href="https://github.com/GodXuxilie/Efficient_ACL_via_RCS">Code</a>] [<a href="https://nips.cc/media/PosterPDFs/NeurIPS%202023/70886.png?t=1701436495.3604662">Poster</a>] [<a href="https://zhuanlan.zhihu.com/p/669541942">知乎 (Chinese Quora)</a>] [<b style="color:red;"><i>Spotlight, top 3.06%</i></b>]
</li>
<li><span id="NIPS23b">Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization.</span> <br/>
<b>Xilie Xu</b><sup>*</sup>, Jingfeng Zhang<sup>*</sup>, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. <br> 
<a href="https://neurips.cc/Conferences/2023"><i>The 37th Annual Conference on Neural Information Processing Systems</i></a> <i>(NeurIPS 2023)</i>,  New Orleans, US, 2023. 
<br/> [<a href="https://arxiv.org/pdf/2305.00374.pdf">PDF</a>] [<a href="https://github.com/GodXuxilie/Enhancing_ACL_via_AIR">Code</a>] [<a href="https://nips.cc/media/PosterPDFs/NeurIPS%202023/69867.png?t=1701436551.2570322">Poster</a>] [<a href="https://robustssl.github.io/">Project Page</a>]
</li>
<li><span id="ICML22">Adversarial Attack and Defense for Non-Parametric Two-Sample Tests.</span> 
<br/>
<b>Xilie Xu</b><sup>*</sup>, Jingfeng Zhang<sup>*</sup>, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. <br> 
<a href="https://icml.cc/Conferences/2022"><i>The 39th International Conference on Machine Learning</i></a> <i>(ICML 2022)</i>, Baltimore, US, 2022. 
<br/> [<a href="https://proceedings.mlr.press/v162/xu22m/xu22m.pdf">PDF</a>] [<a href="https://github.com/GodXuxilie/Robust-TST">Code</a>]
</li>
<li><span id="TMLR22">NoiLin: Improving Adversarial Training and Correcting Stereotype of Noisy Labels. </span> <br>
Jingfeng Zhang<sup>*</sup>, <b>Xilie Xu</b><sup>*</sup>, Bo Han, Tongliang Liu, Lizhen Cui, Gang Niu, Masashi Sugiyama.  <br> 
<a href="https://jmlr.org/tmlr/"><i>Transactions on Machine Learning Research</i></a> <i>(TMLR 2022)</i>. 
<br/> [<a href="https://openreview.net/pdf?id=zlQXV7xtZs">PDF</a>] [<a href="https://github.com/zjfheart/NoiLIn">Code</a>]
</li>
<li><span id="TDSC22">Decision Boundary-aware Data Augmentation for Adversarial Training.</span><br>
Chen Chen<sup>*</sup>, Jingfeng Zhang<sup>*</sup>, <b>Xilie Xu</b>, Lingjuan Lyu, Chaochao Chen, Tianlei Hu, Gang Chen. <br> 
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8858"><i>IEEE Transactions on Dependable and Secure Computing</i></a> <i>(TDSC 2022)</i>. 
<br/> [<a href="https://ieeexplore.ieee.org/abstract/document/9754227">PDF</a>]
</li>
<li><span id="ICML20">Attacks Which Do Not Kill Training Make Adversarial Learning Stronger.</span><br/>
Jingfeng Zhang<sup>*</sup>, <b>Xilie Xu</b><sup>*</sup>, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, Mohan Kankanhalli.  <br> 
<a href="https://icml.cc/Conferences/2020"><i>The 37th International Conference on Machine Learning</i></a> <i>(ICML 2020)</i>, online, 2022. 
<br/> [<a href="https://proceedings.mlr.press/v119/zhang20z/zhang20z.pdf">PDF</a>] [<a href="https://github.com/zjfheart/Friendly-Adversarial-Training">Code</a>]
</li>
</ol>
