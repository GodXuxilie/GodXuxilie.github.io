---
layout: page
permalink: /publications/index.html
title: Research
---

[[**Google Scholar**]](https://scholar.google.com/citations?hl=en&user=zea9MKUAAAAJ) [[**DBLP**]](https://dblp.org/pid/259/2327.html)  [[**Patent**]](https://www.patentguru.com/cn/inventor/%E5%BE%90%E6%9B%A6%E7%83%88) <br/>

## Research Statement
I have a strong interest in both the theories and practical applications of **trustworthy machine learning**, with a particular focus on **adversarial robustness**. 
Overall, my research works lie in the following three categories: <br/>
**(1) Towards developing and utilizing adversarially robust foundation models**: [[Preprint'23a]](#autoRFT) [[NeurIPS23a, *Spotlight*]](#NIPS23a), [[NeurIPS23b]](#NIPS23b). <br/>
**(2) Towards evaluating and enhancing adversarial robustness of AI-powered applications (e.g., statistical tools, LLMs, diffusion models)**: [[Preprint'23b]](#promptattack) [[ICML22]](#ICML22). <br/>
**(3) Towards enhancing supervised adversarial training**: [[TMLR22]](#TMLR22), [[TDSC22]](#TDSC22), [[ICML20]](#ICML20).

<!-- I'm always welcoming the possibility of collaborations. Please feel free to contact me via [email](xuxilie@comp.nus.edu.sg) if you have any appropriate opportunities you'd like to explore. -->

## Preprint
- <span id="autoRFT">AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework.</span> 
<br/> **Xilie Xu**, Jingfeng Zhang, Mohan Kankanhalli. 
<br/> [[PDF]](https://arxiv.org/abs/2310.01818) [[Code]]() [[Project Page]](./project_page/Benchmark_transferability/index.html)

- <span id="promptattack">An LLM can Fool Itself: A Prompt-Based Adversarial Attack.</span> 
<br/> **Xilie Xu**, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. 
<br/> [[PDF]]() [[Code]]() [[Project Page]](./project_page/prompt_attack/)


## Publication
(\* refers to equal contributions)
- <span id="NIPS23a">Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*37th Annual Conference on Neural Information Processing Systems*](https://neurips.cc/Conferences/2023) *(NeurIPS 2023)*, New Orleans, 2023. 
<br/> [[PDF]](https://arxiv.org/pdf/2302.03857.pdf) [[Code]]() [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:E_wpy3GVQbYJ:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkq7I:AFWwaeYAAAAAZRFis7KReHcROHAYnWoiV6gvx6Y&scisig=AFWwaeYAAAAAZRFisyCipyX6BVfYxxDG9kfCWu0&scisf=4&ct=citation&cd=-1&hl=en) [**Spotlight**]

- <span id="NIPS23b">Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*37th Annual Conference on Neural Information Processing Systems*](https://neurips.cc/Conferences/2023) *(NeurIPS 2023)*, New Orleans, 2023. 
<br/> [[PDF]](https://arxiv.org/pdf/2305.00374.pdf) [[Code]]() [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:OluEdScbo14J:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkhpk:AFWwaeYAAAAAZRFinplfA6oeNQMV6GB6ciligwg&scisig=AFWwaeYAAAAAZRFinrFEeKoc-BMy_xfCVD0W_W4&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1) <br/>

- <span id="ICML22">Adversarial Attack and Defense for Non-Parametric Two-Sample Tests.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*39th International Conference on Machine Learning*](https://icml.cc/Conferences/2022) *(ICML 2022)*, Baltimore, 2022. <br/> [[PDF]](https://proceedings.mlr.press/v162/xu22m/xu22m.pdf) [[Code]](https://github.com/GodXuxilie/Robust-TST) [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:2g1wRPv3Id4J:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkYEA:AFWwaeYAAAAAZRFieEDgzxiUY3BXxqs_xZL1MgE&scisig=AFWwaeYAAAAAZRFieI9pa-Q3utJ9CQwwgSiJ31I&scisf=4&ct=citation&cd=-1&hl=en)

- <span id="TMLR22">NoiLin: Improving Adversarial Training and Correcting Stereotype of Noisy Labels. </span> 
<br> Jingfeng Zhang\*, **Xilie Xu\***, Bo Han, Tongliang Liu, Lizhen Cui, Gang Niu, Masashi Sugiyama. 
<br/> [*Transactions on Machine Learning Research*](https://jmlr.org/tmlr/) *(TMLR 2022)*. 
<br/> [[PDF]](https://openreview.net/pdf?id=zlQXV7xtZs) [[Code]](https://github.com/zjfheart/NoiLIn) [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:XgdVUPGCD5oJ:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkfhk:AFWwaeYAAAAAZRFiZhm0iLKjrkkSgMZj9OXzurs&scisig=AFWwaeYAAAAAZRFiZtcATe8SWP8hwQHf88b1n-E&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1)


- <span id="TDSC22">Decision Boundary-aware Data Augmentation for Adversarial Training.</span> 
<br> Chen Chen\*, Jingfeng Zhang\*, **Xilie Xu**, Lingjuan Lyu, Chaochao Chen, Tianlei Hu, Gang Chen. 
<br/> [*IEEE Transactions on Dependable and Secure Computing*](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8858) *(TDSC 2022)*. 
<br/> [[PDF]](https://ieeexplore.ieee.org/abstract/document/9754227) [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:ZJT4e3wdL3YJ:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkUgk:AFWwaeYAAAAAZRFiSgk8vu679Kz5vSH7IboyCFA&scisig=AFWwaeYAAAAAZRFiSk7NLwlZHp3poWY2WEnqIqs&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1)


- <span id="ICML20">Attacks Which Do Not Kill Training Make Adversarial Learning Stronger.</span> 
<br/> Jingfeng Zhang\*, **Xilie Xu\***, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*37th International Conference on Machine Learning*](https://icml.cc/Conferences/2020) *(ICML 2022)*, Online, 2020.  <br/> [[PDF]](https://proceedings.mlr.press/v119/zhang20z/zhang20z.pdf) [[Code]](https://github.com/zjfheart/Friendly-Adversarial-Training) [[BibTeX]](https://scholar.googleusercontent.com/scholar.bib?q=info:ZV_a5WLALAEJ:scholar.google.com/&output=citation&scisdr=ClE57TOnEJa_oLlkCUQ:AFWwaeYAAAAAZRFiEUTG2oLZn2W5XLkSdws1DXM&scisig=AFWwaeYAAAAAZRFiET8IWpCGzhK4DGqoMdS7mXI&scisf=4&ct=citation&cd=-1&hl=en)



