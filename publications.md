---
layout: page
permalink: /publications/index.html
title: Research
---

[[**Google Scholar**]](https://scholar.google.com/citations?hl=en&user=zea9MKUAAAAJ) [[**DBLP**]](https://dblp.org/pid/259/2327.html)  [[**Patent**]](https://www.patentguru.com/cn/inventor/%E5%BE%90%E6%9B%A6%E7%83%88) <br/>

## Research Statement
I have a strong interest in both the theories and practical applications of **trustworthy machine learning**, with a particular focus on **adversarial robustness**. 
Overall, my research works lie in the following three categories: <br/>
**(1) Towards developing and utilizing adversarially robust foundation models**: [[ICLR'24a]](#autoRFT) [[NeurIPS'23a, *Spotlight*]](#NIPS23a), [[NeurIPS'23b]](#NIPS23b). <br/>
**(2) Towards evaluating and enhancing adversarial robustness of AI-powered applications (e.g., statistical tools, LLMs, diffusion models)**: [[ICLR'24b]](#promptattack), [[AAAI'24 Workshop]](#AAAI24_ReLM), [[ICML'22]](#ICML22). <br/>
**(3) Towards enhancing supervised adversarial training**: [[TMLR'22]](#TMLR22), [[TDSC'22]](#TDSC22), [[ICML'20]](#ICML20).

<!-- I'm always welcoming the possibility of collaborations. Please feel free to contact me via [email](xuxilie@comp.nus.edu.sg) if you have any appropriate opportunities you'd like to explore. -->

## Preprint&Workshop Paper

<ol list-style-type="decimal">
<li list-style-type="decimal"><span id="AAAI24_ReLM">AdvGLUE-GPT: Towards Effective and Efficient Robustness Evaluation of Large Language Models.</span> <br/> 
<b>Xilie Xu</b><sup>*</sup>, Keyi Kong<sup>*</sup>, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. <br> 
<a href="https://sites.google.com/vectorinstitute.ai/relm2024/home"><i>AAAI Workshop on Responsible Language Models</i></a>, Vancouver, Canada, 2024.
<br/> [<a href="https://arxiv.org/abs/2310.13345">PDF</a>] [<a href="https://github.com/GodXuxilie/PromptAttack">Code</a>]</li>
</ol>

## Publication
(\* refers to equal contributions) 
<ol>
<li><span id="promptattack">An LLM can Fool Itself: A Prompt-Based Adversarial Attack.</span><br/> 
<b>Xilie Xu</b><sup>*</sup>, Keyi Kong<sup>*</sup>, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. <br> 
<a href="https://sites.google.com/vectorinstitute.ai/relm2024/home"><i>The Twelfth International Conference on Learning Representations</i></a> <i>(ICLR 2024)</i>, Vienna, Austria, 2024.
<br/> [<a href="https://arxiv.org/abs/2310.13345">PDF</a>] [<a href="https://github.com/GodXuxilie/PromptAttack">Code</a>][<a href="https://godxuxilie.github.io/project_page/prompt_attack">Project Page</a>]
</li>
<li><span id="autoRFT">AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework.</span> 
<b>Xilie Xu</b><sup>*</sup>, Keyi Kong<sup>*</sup>, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli. <br> 
<a href="https://sites.google.com/vectorinstitute.ai/relm2024/home"><i>The Twelfth International Conference on Learning Representations</i></a> <i>(ICLR 2024)</i>, Vienna, Austria, 2024.
<br/> [<a href="https://arxiv.org/abs/2310.13345">PDF</a>] [<a href="https://github.com/GodXuxilie/PromptAttack">Code</a>][<a href="https://godxuxilie.github.io/project_page/prompt_attack">Project Page</a>]
</li>
</ol>

<br/>
<br/>
7\. <span id="autoRFT">AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework.</span> 
<br/> **Xilie Xu**, Jingfeng Zhang, Mohan Kankanhalli. 
<br/> [*The Twelfth International Conference on Learning Representations*](https://iclr.cc/Conferences/2024) *(ICLR 2024)*, Vienna, Austria, 2024.
<br/> [[PDF]](https://arxiv.org/abs/2310.01818) [[Code]](https://github.com/GodXuxilie/RobustSSL_Benchmark) 
<br/>
<br/>
6\. <span id="NIPS23a">Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*The 37th Annual Conference on Neural Information Processing Systems*](https://neurips.cc/Conferences/2023) *(NeurIPS 2023)*, New Orleans, US, 2023. 
<br/> [[PDF]](https://arxiv.org/pdf/2302.03857.pdf) [[Code]](https://github.com/GodXuxilie/Efficient_ACL_via_RCS) [[Poster]](https://nips.cc/media/PosterPDFs/NeurIPS%202023/70886.png?t=1701436495.3604662) [[知乎 (Chinese Quora)]](https://zhuanlan.zhihu.com/p/669541942) [**Spotlight**]
<br/>
<br/>
5\. <span id="NIPS23b">Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*The 37th Annual Conference on Neural Information Processing Systems*](https://neurips.cc/Conferences/2023) *(NeurIPS 2023)*, New Orleans, US, 2023. 
<br/> [[PDF]](https://arxiv.org/pdf/2305.00374.pdf) [[Code]](https://github.com/GodXuxilie/Enhancing_ACL_via_AIR) [[Poster]](https://nips.cc/media/PosterPDFs/NeurIPS%202023/69867.png?t=1701436551.2570322) [[Project Page]](https://robustssl.github.io/) 
<br/>
<br/>
4\. <span id="ICML22">Adversarial Attack and Defense for Non-Parametric Two-Sample Tests.</span> 
<br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*The 39th International Conference on Machine Learning*](https://icml.cc/Conferences/2022) *(ICML 2022)*, Baltimore, US, 2022. <br/> [[PDF]](https://proceedings.mlr.press/v162/xu22m/xu22m.pdf) [[Code]](https://github.com/GodXuxilie/Robust-TST)
<br/>
<br/>
3\. <span id="TMLR22">NoiLin: Improving Adversarial Training and Correcting Stereotype of Noisy Labels. </span> 
<br> Jingfeng Zhang\*, **Xilie Xu\***, Bo Han, Tongliang Liu, Lizhen Cui, Gang Niu, Masashi Sugiyama. 
<br/> [*Transactions on Machine Learning Research*](https://jmlr.org/tmlr/) *(TMLR 2022)*. 
<br/> [[PDF]](https://openreview.net/pdf?id=zlQXV7xtZs) [[Code]](https://github.com/zjfheart/NoiLIn) 
<br/>
<br/>
2\. <span id="TDSC22">Decision Boundary-aware Data Augmentation for Adversarial Training.</span> 
<br> Chen Chen\*, Jingfeng Zhang\*, **Xilie Xu**, Lingjuan Lyu, Chaochao Chen, Tianlei Hu, Gang Chen. 
<br/> [*IEEE Transactions on Dependable and Secure Computing*](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8858) *(TDSC 2022)*. 
<br/> [[PDF]](https://ieeexplore.ieee.org/abstract/document/9754227)
<br/>
<br/>
1\. <span id="ICML20">Attacks Which Do Not Kill Training Make Adversarial Learning Stronger.</span> 
<br/> Jingfeng Zhang\*, **Xilie Xu\***, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, Mohan Kankanhalli. 
<br/> [*The 37th International Conference on Machine Learning*](https://icml.cc/Conferences/2020) *(ICML 2020)*, Online, 2020.  <br/> [[PDF]](https://proceedings.mlr.press/v119/zhang20z/zhang20z.pdf) [[Code]](https://github.com/zjfheart/Friendly-Adversarial-Training) <br/>
