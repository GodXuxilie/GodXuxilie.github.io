---
layout: page
permalink: /publications/index.html
title: Research
---

[[**Google scholar**]](https://scholar.google.com/citations?hl=en&user=zea9MKUAAAAJ) [[**DBLP**]](https://dblp.org/pid/259/2327.html)  [[**Patent**]](https://www.patentguru.com/cn/inventor/%E5%BE%90%E6%9B%A6%E7%83%88) <br/>

## Research Statement
I am broadly interested in the theories and applications of **adversarial machine learning**. My current research focuses on developing adversarially robust foundation models, as well as evaluating and enhancing the adversarial robustness of AI-powered applications (e.g., statistical models, LLMs, diffusion models).

<br/>
Overall, my research works lie in the following three categories: <br/>
**(1) Towards developing adversarially robust foundation models**: [[NeurIPS23a, **Spotlight**]](#NIPS23a), [[NeurIPS23b]](#NIPS23b). <br/>
**(2) Towards evaluating and enhancing adversarial robustness of AI-powered applications (e.g., statistical tools, LLMs, diffusion models)**: [[ICML22]](#ICML22). <br/>
**(3) Towards enhancing supervised adversarial training**: [[TMLR22]](#TMLR22), [[TDSC22]](#TDSC22), [[ICML20]](#ICML20).


## Publication
(\* refers to equal contributions)
- <span id="NIPS23a">Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.</span> <br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. <br/> In [*Advances in Neural Information Processing Systems (NeurIPS 2023)*](https://neurips.cc/Conferences/2023), New Orleans, 2023. <br/> [[paper]](https://arxiv.org/pdf/2302.03857.pdf) [[code]]() [**Spotlight**]

- <span id="NIPS23b">Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization.</span> <br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. <br/> In [*Advances in Neural Information Processing Systems (NeurIPS 2023)*](https://neurips.cc/Conferences/2023), New Orleans, 2023. <br/> [[paper]](https://arxiv.org/pdf/2305.00374.pdf) [[code]]()  <br/>

- <span id="ICML22">Adversarial Attack and Defense for Non-Parametric Two-Sample Tests.</span> <br/> **Xilie Xu\***, Jingfeng Zhang\*, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli. <br/> In [*International Conference on Machine Learning (ICML 2022)*](https://icml.cc/Conferences/2022), Baltimore, 2022. <br/> [[paper]](https://proceedings.mlr.press/v162/xu22m/xu22m.pdf) [[code]](https://github.com/GodXuxilie/Robust-TST)

- <span id="TMLR22">NoiLin: Improving Adversarial Training and Correcting Stereotype of Noisy Labels.</span>  <br> Jingfeng Zhang\*, **Xilie Xu\***, Bo Han, Tongliang Liu, Lizhen Cui, Gang Niu, Masashi Sugiyama. <br/> In [*Transactions on Machine Learning Research*](https://jmlr.org/tmlr/), 2022.<br/> [[paper]](https://openreview.net/pdf?id=zlQXV7xtZs) [[code]](https://github.com/zjfheart/NoiLIn)


- <span id="TDSC22">Decision Boundary-aware Data Augmentation for Adversarial Training.</span> <br> Chen Chen\*, Jingfeng Zhang\*, **Xilie Xu**, Lingjuan Lyu, Chaochao Chen, Tianlei Hu, Gang Chen. <br/> In [*IEEE Transactions on Dependable and Secure Computing*](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8858) 2022. <br/> [[paper]](https://ieeexplore.ieee.org/abstract/document/9754227) <br/>



- <span id="ICML20">Attacks Which Do Not Kill Training Make Adversarial Learning Stronger.</span> <br/> Jingfeng Zhang\*, **Xilie Xu\***, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, Mohan Kankanhalli. <br/> In [*International Conference on Machine Learning (ICML 2022)*](https://icml.cc/Conferences/2020), Online, 2020.  <br/> [[paper]](https://proceedings.mlr.press/v119/zhang20z/zhang20z.pdf) [[code]](https://github.com/zjfheart/Friendly-Adversarial-Training) <br/>